{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1bdad30-a998-4774-82ea-12c03fda144f",
   "metadata": {},
   "source": [
    "## MIST HCC Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2a25d0-1d31-40e9-99e7-4eb68e9ddc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install antspyx==0.3.2\n",
    "# !pip install SimpleITK==2.1.1\n",
    "# !pip install -U -q tqdm\n",
    "# !pip install -U -q psutil\n",
    "# !pip install -U -q tensorflow_addons\n",
    "# !pip install -U -q pynvml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d68e72-2ccf-4611-b951-c0ba38fa031a",
   "metadata": {},
   "source": [
    "#### Import the necessary scripts\n",
    "\n",
    "The python file 'runtime.py' is the main workhorse for this pipeline. It calls the preprocessing pipeline, network architecture, and prediction metrics used for the global training and inference pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c769b783-bb87-49f4-95f4-ff4b12add8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Import runtime script\n",
    "from runtime import RunTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25db5205-dbc4-458f-9424-df9e9545b4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_params = {'raw_data_dir': '/tf/data/hccdetection/raw',\n",
    "               'processed_data_dir': '/tf/data/hccdetection/tfrecord',\n",
    "               'base_model_name': 'hccdetection',\n",
    "                'model_dir': '/tf/data/hccdetection/models',\n",
    "                'prediction_dir': '/tf/data/hccdetection/predictions',\n",
    "                'raw_paths_csv': '/tf/data/hccdetection/paths.csv',\n",
    "                'inferred_params': '/tf/data/hccdetection/inferred_params.json',\n",
    "                'results_csv': '/tf/data/hccdetection/results.csv',\n",
    "                'modality': 'mr',\n",
    "                'mask': ['Truth.raw.nii.gz'], \n",
    "                'images': {'Art': ['Art.raw.nii.gz'],'Pre': ['Pre.raw.nii.gz'],'Ven': ['Ven.raw.nii.gz']}, \n",
    "                'labels': [0, 1],\n",
    "                'final_classes': {'Liver': [1]},\n",
    "                'loss': 'dice', \n",
    "                'model': 'hrnet', \n",
    "                'pocket': True, \n",
    "                'gpu': 7}\n",
    "\n",
    "json_file = '/tf/data/hccdetection/user_params.json'\n",
    "with open(json_file, 'w') as outfile: \n",
    "    json.dump(user_params, outfile)\n",
    "\n",
    "# user_params_lits = {'raw_data_dir': '/tf/data/lits/raw/train',\n",
    "#                     'processed_data_dir': '/tf/data/lits/processed/tfrecord',\n",
    "#                     'base_model_name': 'mist_lits_example',\n",
    "#                     'model_dir': '/tf/data/lits/models/mist_example',\n",
    "#                     'prediction_dir': '/tf/data/lits/predictions/mist_example',\n",
    "#                     'raw_paths_csv': '/tf/github/MIST/mist_example/lits_paths.csv',\n",
    "#                     'inferred_params': '/tf/github/MIST/mist_example/lits_inferred_params.json',\n",
    "#                     'results_csv': '/tf/github/MIST/mist_example/lits_results.csv',\n",
    "#                     'modality': 'ct',\n",
    "#                     'mask': ['segmentation'], \n",
    "#                     'images': {'volume': ['volume']}, \n",
    "#                     'labels': [0, 1, 2],\n",
    "#                     'final_classes': {'Liver': [1, 2], 'Tumor': [2]},\n",
    "#                     'loss': 'gdl', \n",
    "#                     'model': 'unet', \n",
    "#                     'pocket': True}\n",
    "\n",
    "json_file = '/tf/data/hccdetection/user_params.json'\n",
    "with open(json_file, 'w') as outfile: \n",
    "    json.dump(user_params, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e403e0-b882-4591-b8cc-e41e1c52f7f2",
   "metadata": {},
   "source": [
    "#### Run MIST pipeline\n",
    "\n",
    "Once you have your input JSON file, simply run command in the following cell to initiate the MIST training and inference pipeline. Enjoy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0987f672-8430-43fa-8e1b-d6d1ddb77b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create runtime instance\n",
    "train = RunTime(json_file)\n",
    "\n",
    "# Run the runtime instance\n",
    "train.run(run_preprocess = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1de4dbc-7596-427d-bda2-bd55a590e04a",
   "metadata": {},
   "source": [
    "#### Quick note about the ```run_preprocess``` parameter\n",
    "\n",
    "The preprocessing pipeline can take quite a while to run depending on the training data. When running the MIST pipeline for the first time, set ```run_preprocess``` to ```True```. However, if you want to try a different 'model' or 'loss' parameter in your input JSON file after your initial run, then set the ```run_preprocess``` to ```False```. This will skip the preprocessing pipeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
